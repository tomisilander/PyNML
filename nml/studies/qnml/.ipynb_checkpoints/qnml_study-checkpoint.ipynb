{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05bffc98",
   "metadata": {},
   "source": [
    "# Studying qNML for Bayesian networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8c7579",
   "metadata": {},
   "source": [
    "NOTICE! This notebook requires networkx, scipy, and numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecf1a1b",
   "metadata": {},
   "source": [
    "## Recap of definitions of NML and qNML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3439f21a",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "nml(D;G) = \\frac{P(D;\\hat\\theta_G(D))}{\\sum_{D'\\sim D} P(D';\\hat\\theta_G(D'))},\n",
    "$$ \n",
    "\n",
    "where $\\theta_G(D)$ denotes the maximum likelihood paramaters for the data $D$ in the graph $G$.\n",
    "\n",
    "For qNML we decompose the graph with $m$ nodes into $m$ parent sets $G=(G_1, \\ldots, G_m)$, and further into families $F=(F_1, \\ldots, F_m)$, where $F_i = G_i \\cup \\{i\\}$. We select the attributes $S$ of the data sets by denoting $D_{S}$.\n",
    "\n",
    "$$\n",
    "qnml(D;G) = \\sum_{i=1}^m \\frac{nml_1(D_{F_i})}  {nml_1(D_{G_i})},\n",
    "$$\n",
    "\n",
    "where we have used a notation $nml_1(D_S)$ to denote NML for a dataset where $D_S$ is collapsed to a one dimensional dataset of size $N$ with $\\prod_{i \\in S} r_i$ possible discrete values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4712ee2d",
   "metadata": {},
   "source": [
    "## A hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c73949",
   "metadata": {},
   "source": [
    "In particular we study the hypothesis that qNML for a given structure never both over- and underestimates NML for all data formats and sizes:\n",
    "Denoting that two data sets $D$ and $D'$ have same format and size by $D \\sim D'$, we hypothesize that:\n",
    "\n",
    "$$\n",
    "\\forall G\\;  \\forall D, D'\\;:  (D \\sim D') \\implies (nml(D;G) \\gt qnml(D;G) \\implies nml(D';G) \\ge qnml(D';G)) \n",
    "$$ and \n",
    "$$\n",
    "\\forall G\\;  \\forall D, D'\\;:  (D \\sim D') \\implies (nml(D;G) \\lt qnml(D;G) \\implies nml(D';G) \\le qnml(D';G)). \n",
    "$$\n",
    "\n",
    "If true, this means that qNML induced error in estimation of NML is a structural property of a graph $G$ and we could find a correction term for a given $G$ such that makes the result closer to the real NML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50efe87a",
   "metadata": {},
   "source": [
    "### todo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df547f1c",
   "metadata": {},
   "source": [
    "- implement going through the datasets (maybe with weights)\n",
    "- implement different structures\n",
    "- implement qnml, should be easy with NML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df85be07",
   "metadata": {},
   "source": [
    "### Generating all possible datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5978aad",
   "metadata": {},
   "source": [
    "The startegy is to generate all possible datavectors and then use integer partions for data size $N$ to generate all possible datasets. Integer partitions are not enough because it does not contain zeros. If we have $Q$ possible data vectors, and if $N$ is divided to $K$ parts, we have to go through all $Q \\choose K$ possible combinations of vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "521cbb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product, combinations, permutations\n",
    "from sympy.utilities.iterables import partitions\n",
    "\n",
    "def gen_possible_data_vectors(rs):\n",
    "    return product(*map(range,rs))\n",
    "\n",
    "def gen_1(partitems, vixset):\n",
    "    if len(partitems) == 0: \n",
    "        yield []\n",
    "        return\n",
    "    items = partitems[:]\n",
    "    (count, mult) = items.pop(0)\n",
    "    for vs in combinations(vixset, mult):\n",
    "        vec0 = [(v,count) for v in vs]\n",
    "        for vec_rest in gen_1(items, vixset - set(vs)):\n",
    "            yield vec0 + vec_rest\n",
    "    \n",
    "def gen_datasets(vectors, N):\n",
    "    Q = len(vectors)\n",
    "    vixs = range(Q)\n",
    "    for partition in partitions(N, m=Q):\n",
    "        # print(partition)\n",
    "        partitems = list(partition.items())\n",
    "        \n",
    "        counts = sum(([k]*n for k,n in partition.items()), [])\n",
    "        # print('counts = ', counts)\n",
    "        K = len(counts)\n",
    "        for vixset in combinations(vixs, K):\n",
    "            # print('c',vixset)\n",
    "            for vec in gen_1(partitems, set(vixset)):\n",
    "                # print(' ', vec)\n",
    "                yield(vec)\n",
    "            # print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cada95",
   "metadata": {},
   "source": [
    "But this will just tell how many vectors of different kinds there are in the dataset. They can still be in different orders number of which is given by a multinomial coeffient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe8903cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import binom\n",
    "import numpy as np\n",
    "from functools import lru_cache\n",
    "@lru_cache\n",
    "def mulco(ks): \n",
    "    ns = np.cumsum(ks)\n",
    "    binoms = binom(ns,ks)\n",
    "    return np.product(binoms)\n",
    "\n",
    "def mulco_D(D):\n",
    "    return mulco(tuple(count for _vix,count in D))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f715eef3",
   "metadata": {},
   "source": [
    "Just checking that the counts are right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e78a5d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35831808.0 35831808\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "total_count = 0\n",
    "N = 7\n",
    "rs = [2, 3, 2]\n",
    "vectors = list(gen_possible_data_vectors(rs))\n",
    "for D in gen_datasets(vectors, N):\n",
    "    total_count += mulco_D(D)\n",
    "print(total_count, np.product(rs)**N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815e108e",
   "metadata": {},
   "source": [
    "## For generating graphs let us just use networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa111f2",
   "metadata": {},
   "source": [
    "But we still need to gather statistics to find maximum likelihoods. For that we have to extract the counts per parent configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5229408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_parents(g):\n",
    "    for n in g:\n",
    "        yield (n, g.predecessors(n))\n",
    "\n",
    "def gen_counts(vectors, D, g):\n",
    "    for (n,ps) in gen_parents(g): # go through nodes and their parents\n",
    "        pset = frozenset(ps)\n",
    "        for vix, count in D:      # go through the data\n",
    "            d = vectors[vix]\n",
    "            v = d[n]\n",
    "            pcfg = frozenset((p,d[p]) for p in pset)\n",
    "            yield ((n,v), pcfg, count)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def collect_counts(vectors, D, g): # could take N and rs\n",
    "    N_vps = [defaultdict(lambda:defaultdict(int)) for _n in g]\n",
    "    for ((n,v), pcfg, c) in gen_counts(vectors, D, g):\n",
    "        N_vps[n][pcfg][v] += c\n",
    "    return N_vps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9590e94e",
   "metadata": {},
   "source": [
    "Bit of testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9defc22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, [2, 3, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20738fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def random_data(vectors, N):\n",
    "    rng = np.random.default_rng()\n",
    "    rints = rng.integers(low=0, high=len(vectors), size=N)\n",
    "    return list(Counter(rints).items())                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bee03362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (6, 1), (4, 1), (10, 2), (7, 1), (11, 1)]\n",
      "(0, 0, 0) 1\n",
      "(1, 0, 0) 1\n",
      "(0, 2, 0) 1\n",
      "(1, 2, 0) 2\n",
      "(1, 0, 1) 1\n",
      "(1, 2, 1) 1\n",
      "\n",
      "node 0 []\n",
      "N([]) = [(0, 2), (1, 5)]\n",
      "node 1 []\n",
      "N([]) = [(0, 3), (2, 4)]\n",
      "node 2 [0, 1]\n",
      "N([0, 0]) = [(0, 1)]\n",
      "N([1, 0]) = [(0, 1), (1, 1)]\n",
      "N([0, 2]) = [(0, 1)]\n",
      "N([1, 2]) = [(0, 2), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "g = nx.DiGraph()\n",
    "g.add_edges_from([(0,2), (1,2)])\n",
    "\n",
    "vectors = list(gen_possible_data_vectors(rs))\n",
    "D = random_data(vectors, N)\n",
    "\n",
    "N_vps = collect_counts(vectors, D, g)\n",
    "\n",
    "print(D)\n",
    "\n",
    "for (vix,c) in D:\n",
    "    print(vectors[vix], c)\n",
    "print()\n",
    "\n",
    "for n,N_vp in enumerate(N_vps):\n",
    "    print(f'node',n, sorted(g.predecessors(n)))\n",
    "    for pcfg, counts in sorted(N_vp.items()):\n",
    "        print(f'N({[v for (p,v) in sorted(pcfg)]}) = {sorted(counts.items())}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f541750",
   "metadata": {},
   "source": [
    "Looks right !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8526f71d",
   "metadata": {},
   "source": [
    "### So now we could get maximum likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1b9a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "def log_ml(N_vps):\n",
    "    res  = 0.0\n",
    "    for N_vp in N_vps:\n",
    "        for counts in N_vp.values():\n",
    "            freqs = np.array(list(counts.values()))\n",
    "            res -= freqs.sum() * entropy(freqs)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fb79733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.264080719004433"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_ml(N_vps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbaf71e",
   "metadata": {},
   "source": [
    "### and NML distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbfc97ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_log_mls(vectors, N, g):\n",
    "    for D in gen_datasets(vectors, N):\n",
    "        N_vps = collect_counts(vectors, D, g)\n",
    "        yield (log_ml(N_vps), mulco_D(D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba2d6c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>,\n",
      "            {-8.317766166719343: 720.0,\n",
      "             -7.7945180229547955: 1152.0,\n",
      "             -6.931471805599453: 288.0,\n",
      "             -6.931471805599452: 2880.0,\n",
      "             -6.408223661834905: 6336.0,\n",
      "             -5.884975518070357: 1152.0,\n",
      "             -5.545177444479562: 2088.0,\n",
      "             -5.021929300715014: 2304.0,\n",
      "             -4.498681156950466: 1344.0,\n",
      "             -4.1588830833596715: 1872.0,\n",
      "             -2.772588722239781: 252.0,\n",
      "             -2.249340578475233: 336.0,\n",
      "             0.0: 12.0})\n"
     ]
    }
   ],
   "source": [
    "def log_mls_counts(vectors, N, g):\n",
    "    log_mls = defaultdict(int)\n",
    "    for lml, c in gen_log_mls(vectors, 4, g):\n",
    "        log_mls[lml] += c\n",
    "    return log_mls\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(log_mls_counts(vectors, 2, g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "017f8696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_nml_denom(vectors, N):\n",
    "    denom = 0\n",
    "    for lml,c in gen_log_mls(vectors, N, g):\n",
    "        denom += np.exp(lml) * c\n",
    "    return np.log(denom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d69b01",
   "metadata": {},
   "source": [
    "## qNML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f619db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from nml import lognml\n",
    "def log_qnml1(freqs, q):    \n",
    "    freqs0 = [0]*(q-len(freqs)) + freqs\n",
    "    return lognml(freqs0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8133f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_qnml(N_vps, rs, g):\n",
    "    qs = [int(np.product([rs[p] for p in g.predecessors(n)])) for n in range(len(g))]\n",
    "    res = 0\n",
    "    for N_vp, r, q in zip(N_vps, rs, qs):\n",
    "        freqs_f = list(chain(*(counts.values() for counts in N_vp.values())))\n",
    "        freqs_p = list(sum(counts.values()) for counts in N_vp.values())\n",
    "        res += log_qnml1(freqs_f, q*r) \n",
    "        res -= log_qnml1(freqs_p,q)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6015a1",
   "metadata": {},
   "source": [
    "## Empirical study of our hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfe7a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_diffs(vectors, g):\n",
    "    \n",
    "    def gen_diffs():\n",
    "        log_denom = log_nml_denom(vectors, N)\n",
    "        for D in gen_datasets(vectors, N):\n",
    "            N_vps = collect_counts(vectors, D, g)\n",
    "            log_nml = log_ml(N_vps) - log_denom\n",
    "            yield log_qnml(N_vps, rs, g) - log_nml\n",
    "            \n",
    "    difflist = np.array(list(gen_diffs()), dtype='float')\n",
    "    return np.min(difflist), np.max(difflist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "069873e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rss(n, max_r):\n",
    "    rrange = range(2,max_r+1)\n",
    "    return list(product(rrange, repeat=n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb8aca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph(*nodges):\n",
    "    g = nx.DiGraph()\n",
    "    for nodge in nodges:\n",
    "        if isinstance(nodge, int):\n",
    "            g.add_node(nodge)\n",
    "        else:\n",
    "            g.add_edge(*nodge)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710870fb",
   "metadata": {},
   "source": [
    "### Try some graphs where there should be no difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c6c14b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 (2, 2, 2, 3) (-8.881784197001252e-16, 8.881784197001252e-16)\n",
      "4 (3, 3, 2, 2) (-3.552713678800501e-15, 1.7763568394002505e-15)\n",
      "3 (3, 3, 2, 3) (-8.384404281969182e-13, -8.348877145181177e-13)\n",
      "4 (3, 2, 3, 3) (-3.552713678800501e-15, 3.552713678800501e-15)\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "\n",
    "graphs = [\n",
    "    get_graph((0,1), (2,3)),\n",
    "    get_graph(0, (1,2), (1,3), (2,3)),\n",
    "    get_graph(0,(1,2),3),\n",
    "    get_graph((0,1),(0,2),(0,3),(1,2),(1,3),(2,3))\n",
    "]\n",
    "\n",
    "for g in graphs:\n",
    "    rss = get_rss(len(g.nodes()),3)\n",
    "    rs = choice(rss)\n",
    "    vectors = list(gen_possible_data_vectors(rs))    \n",
    "    N = choice(list(range(2,5)))\n",
    "    d = get_max_diffs(vectors, g)\n",
    "    print(N, rs, d)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21f5769",
   "metadata": {},
   "source": [
    "### Try some graphs where there should be difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "89cf891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 (3, 3, 3) (0.11778303565638293, 0.11778303565638382)\n",
      "2 (2, 3, 3) (0.08004270767353638, 0.08004270767353727)\n",
      "2 (2, 3, 2, 2) (0.14045183546909623, 0.14045183546909712)\n",
      "2 (2, 2, 2) (-0.033274788884872564, -0.033274788884872564)\n",
      "2 (2, 3, 3, 3) (0.08879549878313053, 0.08879549878313142)\n",
      "2 (2, 3, 3, 3) (0.2719337154836401, 0.27193371548364276)\n",
      "2 (2, 2, 2, 2, 3) (-0.002184201814841913, -0.0021842018148401365)\n",
      "2 (3, 3, 3, 2) (0.2719337154836401, 0.27193371548364276)\n",
      "2 (2, 2, 2, 3, 3) (0.30830135965451433, 0.3083013596545179)\n",
      "2 (2, 2, 2, 2, 3, 3) (0.12413061833981232, 0.12413061833981764)\n"
     ]
    }
   ],
   "source": [
    "graphs = [\n",
    "    get_graph((0,1), (0,2)), # A\n",
    "    get_graph((0,1), (1,2)), # I\n",
    "    get_graph((0,1), (1,2), (2,3)), # long I\n",
    "    get_graph((0,2), (1,2)), # V\n",
    "    get_graph((0,2), (1,2), (2,3)), # Y\n",
    "    get_graph((0,1), (1,2), (1,3)), # Y'\n",
    "    get_graph((0,2), (1,2), (2,4), (3,4)), # W'\n",
    "    get_graph((0,1), (1,2), (1,3), (1,3)), # Y+\n",
    "    get_graph((0,2), (1,2), (2,3), (2,4)), # X\n",
    "    get_graph((0,3), (1,3), (2,3), (2,4), (2,5)), # X^\n",
    "]\n",
    "\n",
    "for g in graphs:\n",
    "    rss = get_rss(len(g.nodes()),3)\n",
    "    rs = choice(rss)\n",
    "    vectors = list(gen_possible_data_vectors(rs))    \n",
    "    N = choice(list(range(2,3)))\n",
    "    d = get_max_diffs(vectors, g)\n",
    "    print(N, rs, d)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494a8698",
   "metadata": {},
   "source": [
    "Running the cell above many times it looks like our hypothesis is true, V and W networks tend to underestimate the NML and the rest overestimate it. This suggests a bias against converging structures when selecting the model by qNML - this serves as a kind of additional complexity penalty "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
